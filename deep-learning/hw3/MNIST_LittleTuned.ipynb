{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f26683ae090>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration - If you have CUDA configured, you must use it. Try training with CPU and observe what happens\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Setting a seed for torch\n",
    "'''step - Your seed will be the last 6 digits of your A# excluding any leading zeros'''\n",
    "torch.manual_seed(278024) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3602.64s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sean/sourcecode/school/deep-learning/hw3\n"
     ]
    }
   ],
   "source": [
    "#Check your Current Working Directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Batch Size\n",
    "'''Step - Set the correct batch size. '''\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "# Download MNIST dataset to local drive. A new folder \"data\" will be created in the current directory to store data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader for shuffling and batching.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters - We are specifying these apriori\n",
    "#Network Architecture\n",
    "input_size = 784\n",
    "'''Output layer has 10 nodes because we want to predict 10 classes'''\n",
    "num_classes = 10 \n",
    "\n",
    "#Training Parameters\n",
    "''' - Define the number of epochs and observe the changes'''\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        '''\n",
    "        Step - Define the N/w architecture. Use RELU Activation\n",
    "        '''\n",
    "        ''' Step - Define a Linear Unit with input size and hidden size''' \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        ''' Step - Define a RELU Activation unit'''\n",
    "        self.relu = nn.ReLU()\n",
    "        ''' Step - Define a Linear Unit with input size and output size (number of classes for MNIST)''' \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Step - Forward Propagate through the layers as defined above. Fill in params in place of ...\n",
    "        '''\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Define the Loss Function and optimizer\n",
    "'''Step - Define a proper loss function'''\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/3000], Loss: 0.5199\n",
      "Epoch [1/10], Step [200/3000], Loss: 0.3954\n",
      "Epoch [1/10], Step [300/3000], Loss: 0.5237\n",
      "Epoch [1/10], Step [400/3000], Loss: 0.2990\n",
      "Epoch [1/10], Step [500/3000], Loss: 0.4709\n",
      "Epoch [1/10], Step [600/3000], Loss: 0.2801\n",
      "Epoch [1/10], Step [700/3000], Loss: 0.1271\n",
      "Epoch [1/10], Step [800/3000], Loss: 0.0724\n",
      "Epoch [1/10], Step [900/3000], Loss: 0.3436\n",
      "Epoch [1/10], Step [1000/3000], Loss: 0.1587\n",
      "Epoch [1/10], Step [1100/3000], Loss: 0.2273\n",
      "Epoch [1/10], Step [1200/3000], Loss: 0.1285\n",
      "Epoch [1/10], Step [1300/3000], Loss: 0.5680\n",
      "Epoch [1/10], Step [1400/3000], Loss: 0.4211\n",
      "Epoch [1/10], Step [1500/3000], Loss: 0.1628\n",
      "Epoch [1/10], Step [1600/3000], Loss: 0.2717\n",
      "Epoch [1/10], Step [1700/3000], Loss: 0.0979\n",
      "Epoch [1/10], Step [1800/3000], Loss: 0.4230\n",
      "Epoch [1/10], Step [1900/3000], Loss: 0.0553\n",
      "Epoch [1/10], Step [2000/3000], Loss: 0.1103\n",
      "Epoch [1/10], Step [2100/3000], Loss: 0.0917\n",
      "Epoch [1/10], Step [2200/3000], Loss: 0.2444\n",
      "Epoch [1/10], Step [2300/3000], Loss: 0.3254\n",
      "Epoch [1/10], Step [2400/3000], Loss: 0.3212\n",
      "Epoch [1/10], Step [2500/3000], Loss: 0.0500\n",
      "Epoch [1/10], Step [2600/3000], Loss: 0.1398\n",
      "Epoch [1/10], Step [2700/3000], Loss: 0.5023\n",
      "Epoch [1/10], Step [2800/3000], Loss: 0.2930\n",
      "Epoch [1/10], Step [2900/3000], Loss: 0.1153\n",
      "Epoch [1/10], Step [3000/3000], Loss: 0.0331\n",
      "Epoch [2/10], Step [100/3000], Loss: 0.0410\n",
      "Epoch [2/10], Step [200/3000], Loss: 0.2201\n",
      "Epoch [2/10], Step [300/3000], Loss: 0.0479\n",
      "Epoch [2/10], Step [400/3000], Loss: 0.3024\n",
      "Epoch [2/10], Step [500/3000], Loss: 0.1018\n",
      "Epoch [2/10], Step [600/3000], Loss: 0.0456\n",
      "Epoch [2/10], Step [700/3000], Loss: 0.1984\n",
      "Epoch [2/10], Step [800/3000], Loss: 0.0418\n",
      "Epoch [2/10], Step [900/3000], Loss: 0.1301\n",
      "Epoch [2/10], Step [1000/3000], Loss: 0.1724\n",
      "Epoch [2/10], Step [1100/3000], Loss: 0.0067\n",
      "Epoch [2/10], Step [1200/3000], Loss: 0.0611\n",
      "Epoch [2/10], Step [1300/3000], Loss: 0.1415\n",
      "Epoch [2/10], Step [1400/3000], Loss: 0.0252\n",
      "Epoch [2/10], Step [1500/3000], Loss: 0.1144\n",
      "Epoch [2/10], Step [1600/3000], Loss: 0.0942\n",
      "Epoch [2/10], Step [1700/3000], Loss: 0.0285\n",
      "Epoch [2/10], Step [1800/3000], Loss: 0.0992\n",
      "Epoch [2/10], Step [1900/3000], Loss: 0.0224\n",
      "Epoch [2/10], Step [2000/3000], Loss: 0.0179\n",
      "Epoch [2/10], Step [2100/3000], Loss: 0.2078\n",
      "Epoch [2/10], Step [2200/3000], Loss: 0.0957\n",
      "Epoch [2/10], Step [2300/3000], Loss: 0.0897\n",
      "Epoch [2/10], Step [2400/3000], Loss: 0.0842\n",
      "Epoch [2/10], Step [2500/3000], Loss: 0.0212\n",
      "Epoch [2/10], Step [2600/3000], Loss: 0.0386\n",
      "Epoch [2/10], Step [2700/3000], Loss: 0.1648\n",
      "Epoch [2/10], Step [2800/3000], Loss: 0.0135\n",
      "Epoch [2/10], Step [2900/3000], Loss: 0.0591\n",
      "Epoch [2/10], Step [3000/3000], Loss: 0.1052\n",
      "Epoch [3/10], Step [100/3000], Loss: 0.1522\n",
      "Epoch [3/10], Step [200/3000], Loss: 0.1093\n",
      "Epoch [3/10], Step [300/3000], Loss: 0.2560\n",
      "Epoch [3/10], Step [400/3000], Loss: 0.0221\n",
      "Epoch [3/10], Step [500/3000], Loss: 0.0473\n",
      "Epoch [3/10], Step [600/3000], Loss: 0.1909\n",
      "Epoch [3/10], Step [700/3000], Loss: 0.0188\n",
      "Epoch [3/10], Step [800/3000], Loss: 0.0088\n",
      "Epoch [3/10], Step [900/3000], Loss: 0.1372\n",
      "Epoch [3/10], Step [1000/3000], Loss: 0.2159\n",
      "Epoch [3/10], Step [1100/3000], Loss: 0.0170\n",
      "Epoch [3/10], Step [1200/3000], Loss: 0.0741\n",
      "Epoch [3/10], Step [1300/3000], Loss: 0.1951\n",
      "Epoch [3/10], Step [1400/3000], Loss: 0.3112\n",
      "Epoch [3/10], Step [1500/3000], Loss: 0.0632\n",
      "Epoch [3/10], Step [1600/3000], Loss: 0.2030\n",
      "Epoch [3/10], Step [1700/3000], Loss: 0.0493\n",
      "Epoch [3/10], Step [1800/3000], Loss: 0.0282\n",
      "Epoch [3/10], Step [1900/3000], Loss: 0.0384\n",
      "Epoch [3/10], Step [2000/3000], Loss: 0.0573\n",
      "Epoch [3/10], Step [2100/3000], Loss: 0.0246\n",
      "Epoch [3/10], Step [2200/3000], Loss: 0.0083\n",
      "Epoch [3/10], Step [2300/3000], Loss: 0.0410\n",
      "Epoch [3/10], Step [2400/3000], Loss: 0.0592\n",
      "Epoch [3/10], Step [2500/3000], Loss: 0.0054\n",
      "Epoch [3/10], Step [2600/3000], Loss: 0.0290\n",
      "Epoch [3/10], Step [2700/3000], Loss: 0.2262\n",
      "Epoch [3/10], Step [2800/3000], Loss: 0.2117\n",
      "Epoch [3/10], Step [2900/3000], Loss: 0.0093\n",
      "Epoch [3/10], Step [3000/3000], Loss: 0.0203\n",
      "Epoch [4/10], Step [100/3000], Loss: 0.0391\n",
      "Epoch [4/10], Step [200/3000], Loss: 0.0553\n",
      "Epoch [4/10], Step [300/3000], Loss: 0.0217\n",
      "Epoch [4/10], Step [400/3000], Loss: 0.0388\n",
      "Epoch [4/10], Step [500/3000], Loss: 0.0190\n",
      "Epoch [4/10], Step [600/3000], Loss: 0.0493\n",
      "Epoch [4/10], Step [700/3000], Loss: 0.2405\n",
      "Epoch [4/10], Step [800/3000], Loss: 0.2283\n",
      "Epoch [4/10], Step [900/3000], Loss: 0.0190\n",
      "Epoch [4/10], Step [1000/3000], Loss: 0.0202\n",
      "Epoch [4/10], Step [1100/3000], Loss: 0.2917\n",
      "Epoch [4/10], Step [1200/3000], Loss: 0.0219\n",
      "Epoch [4/10], Step [1300/3000], Loss: 0.0068\n",
      "Epoch [4/10], Step [1400/3000], Loss: 0.0625\n",
      "Epoch [4/10], Step [1500/3000], Loss: 0.0097\n",
      "Epoch [4/10], Step [1600/3000], Loss: 0.0314\n",
      "Epoch [4/10], Step [1700/3000], Loss: 0.0167\n",
      "Epoch [4/10], Step [1800/3000], Loss: 0.0236\n",
      "Epoch [4/10], Step [1900/3000], Loss: 0.1034\n",
      "Epoch [4/10], Step [2000/3000], Loss: 0.0438\n",
      "Epoch [4/10], Step [2100/3000], Loss: 0.0806\n",
      "Epoch [4/10], Step [2200/3000], Loss: 0.0087\n",
      "Epoch [4/10], Step [2300/3000], Loss: 0.0047\n",
      "Epoch [4/10], Step [2400/3000], Loss: 0.0576\n",
      "Epoch [4/10], Step [2500/3000], Loss: 0.0018\n",
      "Epoch [4/10], Step [2600/3000], Loss: 0.1915\n",
      "Epoch [4/10], Step [2700/3000], Loss: 0.0353\n",
      "Epoch [4/10], Step [2800/3000], Loss: 0.0400\n",
      "Epoch [4/10], Step [2900/3000], Loss: 0.0421\n",
      "Epoch [4/10], Step [3000/3000], Loss: 0.0252\n",
      "Epoch [5/10], Step [100/3000], Loss: 0.0101\n",
      "Epoch [5/10], Step [200/3000], Loss: 0.2341\n",
      "Epoch [5/10], Step [300/3000], Loss: 0.0127\n",
      "Epoch [5/10], Step [400/3000], Loss: 0.0290\n",
      "Epoch [5/10], Step [500/3000], Loss: 0.0030\n",
      "Epoch [5/10], Step [600/3000], Loss: 0.0410\n",
      "Epoch [5/10], Step [700/3000], Loss: 0.0591\n",
      "Epoch [5/10], Step [800/3000], Loss: 0.0772\n",
      "Epoch [5/10], Step [900/3000], Loss: 0.0374\n",
      "Epoch [5/10], Step [1000/3000], Loss: 0.0686\n",
      "Epoch [5/10], Step [1100/3000], Loss: 0.0943\n",
      "Epoch [5/10], Step [1200/3000], Loss: 0.0752\n",
      "Epoch [5/10], Step [1300/3000], Loss: 0.1162\n",
      "Epoch [5/10], Step [1400/3000], Loss: 0.2171\n",
      "Epoch [5/10], Step [1500/3000], Loss: 0.0079\n",
      "Epoch [5/10], Step [1600/3000], Loss: 0.0673\n",
      "Epoch [5/10], Step [1700/3000], Loss: 0.0539\n",
      "Epoch [5/10], Step [1800/3000], Loss: 0.0627\n",
      "Epoch [5/10], Step [1900/3000], Loss: 0.0320\n",
      "Epoch [5/10], Step [2000/3000], Loss: 0.1112\n",
      "Epoch [5/10], Step [2100/3000], Loss: 0.1485\n",
      "Epoch [5/10], Step [2200/3000], Loss: 0.0133\n",
      "Epoch [5/10], Step [2300/3000], Loss: 0.0275\n",
      "Epoch [5/10], Step [2400/3000], Loss: 0.0202\n",
      "Epoch [5/10], Step [2500/3000], Loss: 0.0155\n",
      "Epoch [5/10], Step [2600/3000], Loss: 0.1182\n",
      "Epoch [5/10], Step [2700/3000], Loss: 0.0306\n",
      "Epoch [5/10], Step [2800/3000], Loss: 0.0044\n",
      "Epoch [5/10], Step [2900/3000], Loss: 0.0179\n",
      "Epoch [5/10], Step [3000/3000], Loss: 0.0057\n",
      "Epoch [6/10], Step [100/3000], Loss: 0.0139\n",
      "Epoch [6/10], Step [200/3000], Loss: 0.0129\n",
      "Epoch [6/10], Step [300/3000], Loss: 0.0081\n",
      "Epoch [6/10], Step [400/3000], Loss: 0.0574\n",
      "Epoch [6/10], Step [500/3000], Loss: 0.0087\n",
      "Epoch [6/10], Step [600/3000], Loss: 0.0034\n",
      "Epoch [6/10], Step [700/3000], Loss: 0.0032\n",
      "Epoch [6/10], Step [800/3000], Loss: 0.0358\n",
      "Epoch [6/10], Step [900/3000], Loss: 0.0240\n",
      "Epoch [6/10], Step [1000/3000], Loss: 0.0242\n",
      "Epoch [6/10], Step [1100/3000], Loss: 0.1593\n",
      "Epoch [6/10], Step [1200/3000], Loss: 0.1662\n",
      "Epoch [6/10], Step [1300/3000], Loss: 0.0196\n",
      "Epoch [6/10], Step [1400/3000], Loss: 0.0436\n",
      "Epoch [6/10], Step [1500/3000], Loss: 0.0021\n",
      "Epoch [6/10], Step [1600/3000], Loss: 0.1476\n",
      "Epoch [6/10], Step [1700/3000], Loss: 0.0253\n",
      "Epoch [6/10], Step [1800/3000], Loss: 0.3667\n",
      "Epoch [6/10], Step [1900/3000], Loss: 0.0075\n",
      "Epoch [6/10], Step [2000/3000], Loss: 0.0036\n",
      "Epoch [6/10], Step [2100/3000], Loss: 0.0008\n",
      "Epoch [6/10], Step [2200/3000], Loss: 0.3008\n",
      "Epoch [6/10], Step [2300/3000], Loss: 0.0943\n",
      "Epoch [6/10], Step [2400/3000], Loss: 0.0463\n",
      "Epoch [6/10], Step [2500/3000], Loss: 0.0650\n",
      "Epoch [6/10], Step [2600/3000], Loss: 0.0096\n",
      "Epoch [6/10], Step [2700/3000], Loss: 0.0058\n",
      "Epoch [6/10], Step [2800/3000], Loss: 0.0438\n",
      "Epoch [6/10], Step [2900/3000], Loss: 0.0039\n",
      "Epoch [6/10], Step [3000/3000], Loss: 0.0331\n",
      "Epoch [7/10], Step [100/3000], Loss: 0.0910\n",
      "Epoch [7/10], Step [200/3000], Loss: 0.0080\n",
      "Epoch [7/10], Step [300/3000], Loss: 0.0077\n",
      "Epoch [7/10], Step [400/3000], Loss: 0.0122\n",
      "Epoch [7/10], Step [500/3000], Loss: 0.0095\n",
      "Epoch [7/10], Step [600/3000], Loss: 0.1426\n",
      "Epoch [7/10], Step [700/3000], Loss: 0.0022\n",
      "Epoch [7/10], Step [800/3000], Loss: 0.0191\n",
      "Epoch [7/10], Step [900/3000], Loss: 0.0124\n",
      "Epoch [7/10], Step [1000/3000], Loss: 0.0632\n",
      "Epoch [7/10], Step [1100/3000], Loss: 0.0526\n",
      "Epoch [7/10], Step [1200/3000], Loss: 0.0152\n",
      "Epoch [7/10], Step [1300/3000], Loss: 0.0080\n",
      "Epoch [7/10], Step [1400/3000], Loss: 0.0325\n",
      "Epoch [7/10], Step [1500/3000], Loss: 0.0017\n",
      "Epoch [7/10], Step [1600/3000], Loss: 0.1166\n",
      "Epoch [7/10], Step [1700/3000], Loss: 0.0936\n",
      "Epoch [7/10], Step [1800/3000], Loss: 0.0384\n",
      "Epoch [7/10], Step [1900/3000], Loss: 0.0063\n",
      "Epoch [7/10], Step [2000/3000], Loss: 0.1200\n",
      "Epoch [7/10], Step [2100/3000], Loss: 0.0456\n",
      "Epoch [7/10], Step [2200/3000], Loss: 0.0004\n",
      "Epoch [7/10], Step [2300/3000], Loss: 0.0727\n",
      "Epoch [7/10], Step [2400/3000], Loss: 0.0070\n",
      "Epoch [7/10], Step [2500/3000], Loss: 0.0184\n",
      "Epoch [7/10], Step [2600/3000], Loss: 0.0145\n",
      "Epoch [7/10], Step [2700/3000], Loss: 0.0068\n",
      "Epoch [7/10], Step [2800/3000], Loss: 0.0044\n",
      "Epoch [7/10], Step [2900/3000], Loss: 0.0035\n",
      "Epoch [7/10], Step [3000/3000], Loss: 0.0776\n",
      "Epoch [8/10], Step [100/3000], Loss: 0.0579\n",
      "Epoch [8/10], Step [200/3000], Loss: 0.0107\n",
      "Epoch [8/10], Step [300/3000], Loss: 0.0178\n",
      "Epoch [8/10], Step [400/3000], Loss: 0.0100\n",
      "Epoch [8/10], Step [500/3000], Loss: 0.0029\n",
      "Epoch [8/10], Step [600/3000], Loss: 0.0010\n",
      "Epoch [8/10], Step [700/3000], Loss: 0.0527\n",
      "Epoch [8/10], Step [800/3000], Loss: 0.0073\n",
      "Epoch [8/10], Step [900/3000], Loss: 0.0221\n",
      "Epoch [8/10], Step [1000/3000], Loss: 0.0810\n",
      "Epoch [8/10], Step [1100/3000], Loss: 0.0663\n",
      "Epoch [8/10], Step [1200/3000], Loss: 0.0465\n",
      "Epoch [8/10], Step [1300/3000], Loss: 0.0402\n",
      "Epoch [8/10], Step [1400/3000], Loss: 0.0257\n",
      "Epoch [8/10], Step [1500/3000], Loss: 0.2105\n",
      "Epoch [8/10], Step [1600/3000], Loss: 0.0081\n",
      "Epoch [8/10], Step [1700/3000], Loss: 0.1094\n",
      "Epoch [8/10], Step [1800/3000], Loss: 0.0063\n",
      "Epoch [8/10], Step [1900/3000], Loss: 0.0069\n",
      "Epoch [8/10], Step [2000/3000], Loss: 0.0128\n",
      "Epoch [8/10], Step [2100/3000], Loss: 0.0048\n",
      "Epoch [8/10], Step [2200/3000], Loss: 0.0089\n",
      "Epoch [8/10], Step [2300/3000], Loss: 0.0583\n",
      "Epoch [8/10], Step [2400/3000], Loss: 0.1418\n",
      "Epoch [8/10], Step [2500/3000], Loss: 0.0466\n",
      "Epoch [8/10], Step [2600/3000], Loss: 0.0063\n",
      "Epoch [8/10], Step [2700/3000], Loss: 0.0044\n",
      "Epoch [8/10], Step [2800/3000], Loss: 0.0118\n",
      "Epoch [8/10], Step [2900/3000], Loss: 0.0048\n",
      "Epoch [8/10], Step [3000/3000], Loss: 0.0070\n",
      "Epoch [9/10], Step [100/3000], Loss: 0.0447\n",
      "Epoch [9/10], Step [200/3000], Loss: 0.0349\n",
      "Epoch [9/10], Step [300/3000], Loss: 0.0023\n",
      "Epoch [9/10], Step [400/3000], Loss: 0.0150\n",
      "Epoch [9/10], Step [500/3000], Loss: 0.0023\n",
      "Epoch [9/10], Step [600/3000], Loss: 0.0049\n",
      "Epoch [9/10], Step [700/3000], Loss: 0.0045\n",
      "Epoch [9/10], Step [800/3000], Loss: 0.0605\n",
      "Epoch [9/10], Step [900/3000], Loss: 0.0277\n",
      "Epoch [9/10], Step [1000/3000], Loss: 0.0094\n",
      "Epoch [9/10], Step [1100/3000], Loss: 0.0015\n",
      "Epoch [9/10], Step [1200/3000], Loss: 0.0089\n",
      "Epoch [9/10], Step [1300/3000], Loss: 0.0715\n",
      "Epoch [9/10], Step [1400/3000], Loss: 0.0027\n",
      "Epoch [9/10], Step [1500/3000], Loss: 0.0043\n",
      "Epoch [9/10], Step [1600/3000], Loss: 0.0011\n",
      "Epoch [9/10], Step [1700/3000], Loss: 0.0087\n",
      "Epoch [9/10], Step [1800/3000], Loss: 0.0047\n",
      "Epoch [9/10], Step [1900/3000], Loss: 0.0081\n",
      "Epoch [9/10], Step [2000/3000], Loss: 0.0318\n",
      "Epoch [9/10], Step [2100/3000], Loss: 0.0662\n",
      "Epoch [9/10], Step [2200/3000], Loss: 0.0517\n",
      "Epoch [9/10], Step [2300/3000], Loss: 0.0049\n",
      "Epoch [9/10], Step [2400/3000], Loss: 0.0227\n",
      "Epoch [9/10], Step [2500/3000], Loss: 0.0239\n",
      "Epoch [9/10], Step [2600/3000], Loss: 0.0105\n",
      "Epoch [9/10], Step [2700/3000], Loss: 0.0008\n",
      "Epoch [9/10], Step [2800/3000], Loss: 0.0015\n",
      "Epoch [9/10], Step [2900/3000], Loss: 0.0074\n",
      "Epoch [9/10], Step [3000/3000], Loss: 0.0494\n",
      "Epoch [10/10], Step [100/3000], Loss: 0.0036\n",
      "Epoch [10/10], Step [200/3000], Loss: 0.0130\n",
      "Epoch [10/10], Step [300/3000], Loss: 0.0004\n",
      "Epoch [10/10], Step [400/3000], Loss: 0.0017\n",
      "Epoch [10/10], Step [500/3000], Loss: 0.0026\n",
      "Epoch [10/10], Step [600/3000], Loss: 0.0014\n",
      "Epoch [10/10], Step [700/3000], Loss: 0.0114\n",
      "Epoch [10/10], Step [800/3000], Loss: 0.0048\n",
      "Epoch [10/10], Step [900/3000], Loss: 0.1495\n",
      "Epoch [10/10], Step [1000/3000], Loss: 0.0008\n",
      "Epoch [10/10], Step [1100/3000], Loss: 0.0037\n",
      "Epoch [10/10], Step [1200/3000], Loss: 0.0434\n",
      "Epoch [10/10], Step [1300/3000], Loss: 0.0134\n",
      "Epoch [10/10], Step [1400/3000], Loss: 0.0228\n",
      "Epoch [10/10], Step [1500/3000], Loss: 0.0043\n",
      "Epoch [10/10], Step [1600/3000], Loss: 0.0012\n",
      "Epoch [10/10], Step [1700/3000], Loss: 0.0039\n",
      "Epoch [10/10], Step [1800/3000], Loss: 0.0003\n",
      "Epoch [10/10], Step [1900/3000], Loss: 0.0002\n",
      "Epoch [10/10], Step [2000/3000], Loss: 0.0053\n",
      "Epoch [10/10], Step [2100/3000], Loss: 0.0006\n",
      "Epoch [10/10], Step [2200/3000], Loss: 0.0139\n",
      "Epoch [10/10], Step [2300/3000], Loss: 0.0011\n",
      "Epoch [10/10], Step [2400/3000], Loss: 0.0019\n",
      "Epoch [10/10], Step [2500/3000], Loss: 0.0006\n",
      "Epoch [10/10], Step [2600/3000], Loss: 0.0072\n",
      "Epoch [10/10], Step [2700/3000], Loss: 0.0030\n",
      "Epoch [10/10], Step [2800/3000], Loss: 0.1233\n",
      "Epoch [10/10], Step [2900/3000], Loss: 0.0032\n",
      "Epoch [10/10], Step [3000/3000], Loss: 0.0341\n",
      "Accuracy of the network on the 10000 test images: 97.83 %, with learning rate: 0.1, and 100 hidden neurons\n",
      "Epoch [1/10], Step [100/3000], Loss: 0.5565\n",
      "Epoch [1/10], Step [200/3000], Loss: 0.1876\n",
      "Epoch [1/10], Step [300/3000], Loss: 0.3006\n",
      "Epoch [1/10], Step [400/3000], Loss: 0.1097\n",
      "Epoch [1/10], Step [500/3000], Loss: 0.1231\n",
      "Epoch [1/10], Step [600/3000], Loss: 0.2016\n",
      "Epoch [1/10], Step [700/3000], Loss: 0.2140\n",
      "Epoch [1/10], Step [800/3000], Loss: 0.1085\n",
      "Epoch [1/10], Step [900/3000], Loss: 0.0734\n",
      "Epoch [1/10], Step [1000/3000], Loss: 0.5719\n",
      "Epoch [1/10], Step [1100/3000], Loss: 0.1586\n",
      "Epoch [1/10], Step [1200/3000], Loss: 0.3193\n",
      "Epoch [1/10], Step [1300/3000], Loss: 0.2281\n",
      "Epoch [1/10], Step [1400/3000], Loss: 0.2747\n",
      "Epoch [1/10], Step [1500/3000], Loss: 0.1526\n",
      "Epoch [1/10], Step [1600/3000], Loss: 0.0540\n",
      "Epoch [1/10], Step [1700/3000], Loss: 0.1430\n",
      "Epoch [1/10], Step [1800/3000], Loss: 0.0561\n",
      "Epoch [1/10], Step [1900/3000], Loss: 0.0796\n",
      "Epoch [1/10], Step [2000/3000], Loss: 0.0692\n",
      "Epoch [1/10], Step [2100/3000], Loss: 0.1561\n",
      "Epoch [1/10], Step [2200/3000], Loss: 0.0772\n",
      "Epoch [1/10], Step [2300/3000], Loss: 0.1031\n",
      "Epoch [1/10], Step [2400/3000], Loss: 0.0481\n",
      "Epoch [1/10], Step [2500/3000], Loss: 0.0158\n",
      "Epoch [1/10], Step [2600/3000], Loss: 0.3416\n",
      "Epoch [1/10], Step [2700/3000], Loss: 0.1550\n",
      "Epoch [1/10], Step [2800/3000], Loss: 0.0389\n",
      "Epoch [1/10], Step [2900/3000], Loss: 0.0867\n",
      "Epoch [1/10], Step [3000/3000], Loss: 0.1964\n",
      "Epoch [2/10], Step [100/3000], Loss: 0.0755\n",
      "Epoch [2/10], Step [200/3000], Loss: 0.4810\n",
      "Epoch [2/10], Step [300/3000], Loss: 0.0336\n",
      "Epoch [2/10], Step [400/3000], Loss: 0.0634\n",
      "Epoch [2/10], Step [500/3000], Loss: 0.2414\n",
      "Epoch [2/10], Step [600/3000], Loss: 0.6126\n",
      "Epoch [2/10], Step [700/3000], Loss: 0.1055\n",
      "Epoch [2/10], Step [800/3000], Loss: 0.0252\n",
      "Epoch [2/10], Step [900/3000], Loss: 0.0198\n",
      "Epoch [2/10], Step [1000/3000], Loss: 0.1306\n",
      "Epoch [2/10], Step [1100/3000], Loss: 0.1719\n",
      "Epoch [2/10], Step [1200/3000], Loss: 0.4392\n",
      "Epoch [2/10], Step [1300/3000], Loss: 0.0459\n",
      "Epoch [2/10], Step [1400/3000], Loss: 0.1918\n",
      "Epoch [2/10], Step [1500/3000], Loss: 0.1049\n",
      "Epoch [2/10], Step [1600/3000], Loss: 0.0552\n",
      "Epoch [2/10], Step [1700/3000], Loss: 0.0086\n",
      "Epoch [2/10], Step [1800/3000], Loss: 0.0734\n",
      "Epoch [2/10], Step [1900/3000], Loss: 0.1130\n",
      "Epoch [2/10], Step [2000/3000], Loss: 0.0163\n",
      "Epoch [2/10], Step [2100/3000], Loss: 0.0846\n",
      "Epoch [2/10], Step [2200/3000], Loss: 0.2468\n",
      "Epoch [2/10], Step [2300/3000], Loss: 0.0211\n",
      "Epoch [2/10], Step [2400/3000], Loss: 0.0130\n",
      "Epoch [2/10], Step [2500/3000], Loss: 0.0129\n",
      "Epoch [2/10], Step [2600/3000], Loss: 0.0728\n",
      "Epoch [2/10], Step [2700/3000], Loss: 0.0140\n",
      "Epoch [2/10], Step [2800/3000], Loss: 0.2854\n",
      "Epoch [2/10], Step [2900/3000], Loss: 0.0741\n",
      "Epoch [2/10], Step [3000/3000], Loss: 0.1405\n",
      "Epoch [3/10], Step [100/3000], Loss: 0.0811\n",
      "Epoch [3/10], Step [200/3000], Loss: 0.0414\n",
      "Epoch [3/10], Step [300/3000], Loss: 0.0895\n",
      "Epoch [3/10], Step [400/3000], Loss: 0.0636\n",
      "Epoch [3/10], Step [500/3000], Loss: 0.0375\n",
      "Epoch [3/10], Step [600/3000], Loss: 0.0115\n",
      "Epoch [3/10], Step [700/3000], Loss: 0.0233\n",
      "Epoch [3/10], Step [800/3000], Loss: 0.2188\n",
      "Epoch [3/10], Step [900/3000], Loss: 0.2805\n",
      "Epoch [3/10], Step [1000/3000], Loss: 0.0644\n",
      "Epoch [3/10], Step [1100/3000], Loss: 0.0336\n",
      "Epoch [3/10], Step [1200/3000], Loss: 0.0408\n",
      "Epoch [3/10], Step [1300/3000], Loss: 0.0075\n",
      "Epoch [3/10], Step [1400/3000], Loss: 0.0500\n",
      "Epoch [3/10], Step [1500/3000], Loss: 0.0871\n",
      "Epoch [3/10], Step [1600/3000], Loss: 0.1894\n",
      "Epoch [3/10], Step [1700/3000], Loss: 0.1260\n",
      "Epoch [3/10], Step [1800/3000], Loss: 0.1165\n",
      "Epoch [3/10], Step [1900/3000], Loss: 0.0066\n",
      "Epoch [3/10], Step [2000/3000], Loss: 0.0163\n",
      "Epoch [3/10], Step [2100/3000], Loss: 0.0243\n",
      "Epoch [3/10], Step [2200/3000], Loss: 0.0225\n",
      "Epoch [3/10], Step [2300/3000], Loss: 0.0215\n",
      "Epoch [3/10], Step [2400/3000], Loss: 0.0113\n",
      "Epoch [3/10], Step [2500/3000], Loss: 0.0494\n",
      "Epoch [3/10], Step [2600/3000], Loss: 0.0181\n",
      "Epoch [3/10], Step [2700/3000], Loss: 0.0477\n",
      "Epoch [3/10], Step [2800/3000], Loss: 0.0051\n",
      "Epoch [3/10], Step [2900/3000], Loss: 0.0487\n",
      "Epoch [3/10], Step [3000/3000], Loss: 0.2594\n",
      "Epoch [4/10], Step [100/3000], Loss: 0.0437\n",
      "Epoch [4/10], Step [200/3000], Loss: 0.0194\n",
      "Epoch [4/10], Step [300/3000], Loss: 0.0573\n",
      "Epoch [4/10], Step [400/3000], Loss: 0.0086\n",
      "Epoch [4/10], Step [500/3000], Loss: 0.0061\n",
      "Epoch [4/10], Step [600/3000], Loss: 0.0233\n",
      "Epoch [4/10], Step [700/3000], Loss: 0.0041\n",
      "Epoch [4/10], Step [800/3000], Loss: 0.3890\n",
      "Epoch [4/10], Step [900/3000], Loss: 0.0191\n",
      "Epoch [4/10], Step [1000/3000], Loss: 0.0137\n",
      "Epoch [4/10], Step [1100/3000], Loss: 0.0040\n",
      "Epoch [4/10], Step [1200/3000], Loss: 0.0946\n",
      "Epoch [4/10], Step [1300/3000], Loss: 0.0611\n",
      "Epoch [4/10], Step [1400/3000], Loss: 0.0586\n",
      "Epoch [4/10], Step [1500/3000], Loss: 0.0061\n",
      "Epoch [4/10], Step [1600/3000], Loss: 0.0026\n",
      "Epoch [4/10], Step [1700/3000], Loss: 0.1772\n",
      "Epoch [4/10], Step [1800/3000], Loss: 0.0205\n",
      "Epoch [4/10], Step [1900/3000], Loss: 0.0105\n",
      "Epoch [4/10], Step [2000/3000], Loss: 0.0295\n",
      "Epoch [4/10], Step [2100/3000], Loss: 0.0504\n",
      "Epoch [4/10], Step [2200/3000], Loss: 0.0096\n",
      "Epoch [4/10], Step [2300/3000], Loss: 0.0114\n",
      "Epoch [4/10], Step [2400/3000], Loss: 0.0154\n",
      "Epoch [4/10], Step [2500/3000], Loss: 0.0441\n",
      "Epoch [4/10], Step [2600/3000], Loss: 0.0757\n",
      "Epoch [4/10], Step [2700/3000], Loss: 0.0254\n",
      "Epoch [4/10], Step [2800/3000], Loss: 0.1091\n",
      "Epoch [4/10], Step [2900/3000], Loss: 0.0346\n",
      "Epoch [4/10], Step [3000/3000], Loss: 0.0301\n",
      "Epoch [5/10], Step [100/3000], Loss: 0.0031\n",
      "Epoch [5/10], Step [200/3000], Loss: 0.0234\n",
      "Epoch [5/10], Step [300/3000], Loss: 0.0038\n",
      "Epoch [5/10], Step [400/3000], Loss: 0.0228\n",
      "Epoch [5/10], Step [500/3000], Loss: 0.0533\n",
      "Epoch [5/10], Step [600/3000], Loss: 0.0190\n",
      "Epoch [5/10], Step [700/3000], Loss: 0.0302\n",
      "Epoch [5/10], Step [800/3000], Loss: 0.0423\n",
      "Epoch [5/10], Step [900/3000], Loss: 0.0419\n",
      "Epoch [5/10], Step [1000/3000], Loss: 0.0014\n",
      "Epoch [5/10], Step [1100/3000], Loss: 0.0304\n",
      "Epoch [5/10], Step [1200/3000], Loss: 0.0407\n",
      "Epoch [5/10], Step [1300/3000], Loss: 0.0293\n",
      "Epoch [5/10], Step [1400/3000], Loss: 0.2281\n",
      "Epoch [5/10], Step [1500/3000], Loss: 0.0102\n",
      "Epoch [5/10], Step [1600/3000], Loss: 0.0334\n",
      "Epoch [5/10], Step [1700/3000], Loss: 0.1740\n",
      "Epoch [5/10], Step [1800/3000], Loss: 0.1476\n",
      "Epoch [5/10], Step [1900/3000], Loss: 0.0372\n",
      "Epoch [5/10], Step [2000/3000], Loss: 0.0627\n",
      "Epoch [5/10], Step [2100/3000], Loss: 0.0044\n",
      "Epoch [5/10], Step [2200/3000], Loss: 0.0683\n",
      "Epoch [5/10], Step [2300/3000], Loss: 0.0040\n",
      "Epoch [5/10], Step [2400/3000], Loss: 0.0554\n",
      "Epoch [5/10], Step [2500/3000], Loss: 0.0884\n",
      "Epoch [5/10], Step [2600/3000], Loss: 0.0111\n",
      "Epoch [5/10], Step [2700/3000], Loss: 0.0055\n",
      "Epoch [5/10], Step [2800/3000], Loss: 0.0127\n",
      "Epoch [5/10], Step [2900/3000], Loss: 0.0123\n",
      "Epoch [5/10], Step [3000/3000], Loss: 0.0060\n",
      "Epoch [6/10], Step [100/3000], Loss: 0.0087\n",
      "Epoch [6/10], Step [200/3000], Loss: 0.0825\n",
      "Epoch [6/10], Step [300/3000], Loss: 0.0029\n",
      "Epoch [6/10], Step [400/3000], Loss: 0.0689\n",
      "Epoch [6/10], Step [500/3000], Loss: 0.0298\n",
      "Epoch [6/10], Step [600/3000], Loss: 0.0373\n",
      "Epoch [6/10], Step [700/3000], Loss: 0.0036\n",
      "Epoch [6/10], Step [800/3000], Loss: 0.0168\n",
      "Epoch [6/10], Step [900/3000], Loss: 0.0477\n",
      "Epoch [6/10], Step [1000/3000], Loss: 0.0028\n",
      "Epoch [6/10], Step [1100/3000], Loss: 0.0063\n",
      "Epoch [6/10], Step [1200/3000], Loss: 0.0124\n",
      "Epoch [6/10], Step [1300/3000], Loss: 0.0470\n",
      "Epoch [6/10], Step [1400/3000], Loss: 0.0074\n",
      "Epoch [6/10], Step [1500/3000], Loss: 0.5467\n",
      "Epoch [6/10], Step [1600/3000], Loss: 0.0275\n",
      "Epoch [6/10], Step [1700/3000], Loss: 0.0033\n",
      "Epoch [6/10], Step [1800/3000], Loss: 0.0073\n",
      "Epoch [6/10], Step [1900/3000], Loss: 0.0205\n",
      "Epoch [6/10], Step [2000/3000], Loss: 0.0016\n",
      "Epoch [6/10], Step [2100/3000], Loss: 0.0226\n",
      "Epoch [6/10], Step [2200/3000], Loss: 0.0030\n",
      "Epoch [6/10], Step [2300/3000], Loss: 0.0259\n",
      "Epoch [6/10], Step [2400/3000], Loss: 0.0126\n",
      "Epoch [6/10], Step [2500/3000], Loss: 0.0065\n",
      "Epoch [6/10], Step [2600/3000], Loss: 0.0037\n",
      "Epoch [6/10], Step [2700/3000], Loss: 0.0076\n",
      "Epoch [6/10], Step [2800/3000], Loss: 0.0844\n",
      "Epoch [6/10], Step [2900/3000], Loss: 0.1495\n",
      "Epoch [6/10], Step [3000/3000], Loss: 0.0041\n",
      "Epoch [7/10], Step [100/3000], Loss: 0.0065\n",
      "Epoch [7/10], Step [200/3000], Loss: 0.0233\n",
      "Epoch [7/10], Step [300/3000], Loss: 0.0371\n",
      "Epoch [7/10], Step [400/3000], Loss: 0.0408\n",
      "Epoch [7/10], Step [500/3000], Loss: 0.0105\n",
      "Epoch [7/10], Step [600/3000], Loss: 0.0107\n",
      "Epoch [7/10], Step [700/3000], Loss: 0.0024\n",
      "Epoch [7/10], Step [800/3000], Loss: 0.0211\n",
      "Epoch [7/10], Step [900/3000], Loss: 0.0027\n",
      "Epoch [7/10], Step [1000/3000], Loss: 0.0431\n",
      "Epoch [7/10], Step [1100/3000], Loss: 0.0230\n",
      "Epoch [7/10], Step [1200/3000], Loss: 0.0125\n",
      "Epoch [7/10], Step [1300/3000], Loss: 0.0125\n",
      "Epoch [7/10], Step [1400/3000], Loss: 0.0191\n",
      "Epoch [7/10], Step [1500/3000], Loss: 0.0047\n",
      "Epoch [7/10], Step [1600/3000], Loss: 0.0033\n",
      "Epoch [7/10], Step [1700/3000], Loss: 0.0171\n",
      "Epoch [7/10], Step [1800/3000], Loss: 0.0056\n",
      "Epoch [7/10], Step [1900/3000], Loss: 0.0022\n",
      "Epoch [7/10], Step [2000/3000], Loss: 0.0081\n",
      "Epoch [7/10], Step [2100/3000], Loss: 0.0059\n",
      "Epoch [7/10], Step [2200/3000], Loss: 0.0436\n",
      "Epoch [7/10], Step [2300/3000], Loss: 0.0046\n",
      "Epoch [7/10], Step [2400/3000], Loss: 0.0312\n",
      "Epoch [7/10], Step [2500/3000], Loss: 0.0442\n",
      "Epoch [7/10], Step [2600/3000], Loss: 0.0206\n",
      "Epoch [7/10], Step [2700/3000], Loss: 0.0320\n",
      "Epoch [7/10], Step [2800/3000], Loss: 0.0248\n",
      "Epoch [7/10], Step [2900/3000], Loss: 0.0252\n",
      "Epoch [7/10], Step [3000/3000], Loss: 0.0151\n",
      "Epoch [8/10], Step [100/3000], Loss: 0.0149\n",
      "Epoch [8/10], Step [200/3000], Loss: 0.0019\n",
      "Epoch [8/10], Step [300/3000], Loss: 0.0078\n",
      "Epoch [8/10], Step [400/3000], Loss: 0.0023\n",
      "Epoch [8/10], Step [500/3000], Loss: 0.0025\n",
      "Epoch [8/10], Step [600/3000], Loss: 0.0052\n",
      "Epoch [8/10], Step [700/3000], Loss: 0.0406\n",
      "Epoch [8/10], Step [800/3000], Loss: 0.0178\n",
      "Epoch [8/10], Step [900/3000], Loss: 0.0127\n",
      "Epoch [8/10], Step [1000/3000], Loss: 0.0115\n",
      "Epoch [8/10], Step [1100/3000], Loss: 0.0441\n",
      "Epoch [8/10], Step [1200/3000], Loss: 0.0062\n",
      "Epoch [8/10], Step [1300/3000], Loss: 0.0411\n",
      "Epoch [8/10], Step [1400/3000], Loss: 0.0007\n",
      "Epoch [8/10], Step [1500/3000], Loss: 0.0013\n",
      "Epoch [8/10], Step [1600/3000], Loss: 0.0047\n",
      "Epoch [8/10], Step [1700/3000], Loss: 0.0055\n",
      "Epoch [8/10], Step [1800/3000], Loss: 0.0005\n",
      "Epoch [8/10], Step [1900/3000], Loss: 0.0043\n",
      "Epoch [8/10], Step [2000/3000], Loss: 0.0675\n",
      "Epoch [8/10], Step [2100/3000], Loss: 0.0416\n",
      "Epoch [8/10], Step [2200/3000], Loss: 0.0301\n",
      "Epoch [8/10], Step [2300/3000], Loss: 0.0077\n",
      "Epoch [8/10], Step [2400/3000], Loss: 0.0145\n",
      "Epoch [8/10], Step [2500/3000], Loss: 0.0052\n",
      "Epoch [8/10], Step [2600/3000], Loss: 0.0030\n",
      "Epoch [8/10], Step [2700/3000], Loss: 0.0056\n",
      "Epoch [8/10], Step [2800/3000], Loss: 0.0033\n",
      "Epoch [8/10], Step [2900/3000], Loss: 0.0093\n",
      "Epoch [8/10], Step [3000/3000], Loss: 0.0062\n",
      "Epoch [9/10], Step [100/3000], Loss: 0.0211\n",
      "Epoch [9/10], Step [200/3000], Loss: 0.0180\n",
      "Epoch [9/10], Step [300/3000], Loss: 0.0059\n",
      "Epoch [9/10], Step [400/3000], Loss: 0.0174\n",
      "Epoch [9/10], Step [500/3000], Loss: 0.0251\n",
      "Epoch [9/10], Step [600/3000], Loss: 0.0179\n",
      "Epoch [9/10], Step [700/3000], Loss: 0.0004\n",
      "Epoch [9/10], Step [800/3000], Loss: 0.0195\n",
      "Epoch [9/10], Step [900/3000], Loss: 0.0144\n",
      "Epoch [9/10], Step [1000/3000], Loss: 0.0003\n",
      "Epoch [9/10], Step [1100/3000], Loss: 0.0091\n",
      "Epoch [9/10], Step [1200/3000], Loss: 0.0249\n",
      "Epoch [9/10], Step [1300/3000], Loss: 0.0132\n",
      "Epoch [9/10], Step [1400/3000], Loss: 0.0250\n",
      "Epoch [9/10], Step [1500/3000], Loss: 0.0058\n",
      "Epoch [9/10], Step [1600/3000], Loss: 0.0121\n",
      "Epoch [9/10], Step [1700/3000], Loss: 0.0360\n",
      "Epoch [9/10], Step [1800/3000], Loss: 0.0018\n",
      "Epoch [9/10], Step [1900/3000], Loss: 0.0011\n",
      "Epoch [9/10], Step [2000/3000], Loss: 0.0122\n",
      "Epoch [9/10], Step [2100/3000], Loss: 0.0253\n",
      "Epoch [9/10], Step [2200/3000], Loss: 0.0007\n",
      "Epoch [9/10], Step [2300/3000], Loss: 0.0097\n",
      "Epoch [9/10], Step [2400/3000], Loss: 0.0207\n",
      "Epoch [9/10], Step [2500/3000], Loss: 0.0084\n",
      "Epoch [9/10], Step [2600/3000], Loss: 0.0070\n",
      "Epoch [9/10], Step [2700/3000], Loss: 0.0010\n",
      "Epoch [9/10], Step [2800/3000], Loss: 0.0100\n",
      "Epoch [9/10], Step [2900/3000], Loss: 0.0074\n",
      "Epoch [9/10], Step [3000/3000], Loss: 0.0106\n",
      "Epoch [10/10], Step [100/3000], Loss: 0.0159\n",
      "Epoch [10/10], Step [200/3000], Loss: 0.0049\n",
      "Epoch [10/10], Step [300/3000], Loss: 0.0004\n",
      "Epoch [10/10], Step [400/3000], Loss: 0.0043\n",
      "Epoch [10/10], Step [500/3000], Loss: 0.0013\n",
      "Epoch [10/10], Step [600/3000], Loss: 0.0060\n",
      "Epoch [10/10], Step [700/3000], Loss: 0.0037\n",
      "Epoch [10/10], Step [800/3000], Loss: 0.0044\n",
      "Epoch [10/10], Step [900/3000], Loss: 0.0108\n",
      "Epoch [10/10], Step [1000/3000], Loss: 0.0042\n",
      "Epoch [10/10], Step [1100/3000], Loss: 0.0106\n",
      "Epoch [10/10], Step [1200/3000], Loss: 0.0036\n",
      "Epoch [10/10], Step [1300/3000], Loss: 0.0169\n",
      "Epoch [10/10], Step [1400/3000], Loss: 0.0084\n",
      "Epoch [10/10], Step [1500/3000], Loss: 0.0145\n",
      "Epoch [10/10], Step [1600/3000], Loss: 0.0746\n",
      "Epoch [10/10], Step [1700/3000], Loss: 0.0009\n",
      "Epoch [10/10], Step [1800/3000], Loss: 0.0081\n",
      "Epoch [10/10], Step [1900/3000], Loss: 0.0049\n",
      "Epoch [10/10], Step [2000/3000], Loss: 0.0076\n",
      "Epoch [10/10], Step [2100/3000], Loss: 0.0144\n",
      "Epoch [10/10], Step [2200/3000], Loss: 0.0012\n",
      "Epoch [10/10], Step [2300/3000], Loss: 0.0480\n",
      "Epoch [10/10], Step [2400/3000], Loss: 0.0019\n",
      "Epoch [10/10], Step [2500/3000], Loss: 0.0052\n",
      "Epoch [10/10], Step [2600/3000], Loss: 0.0013\n",
      "Epoch [10/10], Step [2700/3000], Loss: 0.0010\n",
      "Epoch [10/10], Step [2800/3000], Loss: 0.0061\n",
      "Epoch [10/10], Step [2900/3000], Loss: 0.0017\n",
      "Epoch [10/10], Step [3000/3000], Loss: 0.0697\n",
      "Accuracy of the network on the 10000 test images: 98.27 %, with learning rate: 0.1, and 1000 hidden neurons\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\"\"\"step - choose different learning rates and store them in a list and observe the changes \"\"\"\n",
    "learning_rates = [.1]\n",
    "\"\"\"Vary this number and observe the changes, define a list of possible values\"\"\"\n",
    "hidden_sizes = [100, 1000] \n",
    "\n",
    "\n",
    "for hidden_size, learning_rate in itertools.product(hidden_sizes, learning_rates):\n",
    "    total_step = len(train_loader)\n",
    "    #Define the Model Object and your optimizer\n",
    "    model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "    \"\"\"Step - Invoke an appropriate optimizer that takes a ...?\"\"\"  \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            \"\"\"Step - Get Network outputs with forward propagation with current network weights\"\"\"\n",
    "            outputs = model(images)\n",
    "            \"\"\"Step - Get Loss by comparing outputs with True Labels after forward propagation\"\"\"\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "\n",
    "            \"\"\"Step - ... below needs to be replaced with functions\"\"\"\n",
    "            \"\"\"Step - clear the gradients after each pass - Strongly recommended\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\"Backpropagate the Loss to calculate gradient for each weight\"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\"Update the weight using the learning rate\"\"\"\n",
    "            optimizer.step()\n",
    "\n",
    "            #Print Progress every 100 steps\n",
    "            if (i+1) % 100 == 0:\n",
    "                print (\n",
    "                    f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], \"\n",
    "                    f\"Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "                \n",
    "    # Test the model once you finish training \n",
    "\n",
    "    with torch.no_grad(): # In test phase, we don\"t need to compute gradients (for memory efficiency)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            \"\"\"Step - Move images to device after appropriate reshaping\"\"\"\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            \"\"\"Step  - Move labels to device\"\"\"\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            #get network outputs\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy of the network on the 10000 test images: {(100 * correct / total)} %, with learning rate: {learning_rate}, and {hidden_size} hidden neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint for future use\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d34b9460366b78c854f620c66445fee9afeecf972eedca0a7e75b88f83987c10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
